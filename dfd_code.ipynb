{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC28TBJlMGRX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount ('\\content\\drive')#change the google drive path"
      ],
      "metadata": {
        "id": "uKdrZ7FVMOWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "tbTU0SA6MGRX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "RLEC-lxsMGRX"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "dataset_path = '/kaggle/input/1000-videos-split/1000_videos'#change the data set path\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "validation_path = os.path.join(dataset_path, 'validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "e87m4PTTMGRY"
      },
      "outputs": [],
      "source": [
        "# Define image size and channels\n",
        "Image_Size = 180  # Update this as necessary\n",
        "Channels = 3\n",
        "Epochs = 100 # Update this as necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "DJ7rP1thMGRY"
      },
      "outputs": [],
      "source": [
        "# Create a function to load and preprocess images\n",
        "def load_images(image_path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for img in os.listdir(image_path):\n",
        "        img_path = os.path.join(image_path, img)\n",
        "        img_data = cv2.imread(img_path)\n",
        "        img_data = cv2.resize(img_data, (Image_Size, Image_Size))\n",
        "        images.append(img_data)\n",
        "        labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "0Lji8i6ZMGRY"
      },
      "outputs": [],
      "source": [
        "# Load training data\n",
        "train_real_images, train_real_labels = load_images('/kaggle/input/1000-videos-split/1000_videos/train/real', 0) #change the path\n",
        "train_fake_images, train_fake_labels = load_images('/kaggle/input/1000-videos-split/1000_videos/train/fake', 1) #change the path\n",
        "\n",
        "# Load validation data\n",
        "validation_real_images, validation_real_labels = load_images('/kaggle/input/1000-videos-split/1000_videos/validation/real', 0) #change the path\n",
        "validation_fake_images, validation_fake_labels = load_images('/kaggle/input/1000-videos-split/1000_videos/validation/fake', 1) #change the path\n",
        "\n",
        "# Load test data  # Load the test data here\n",
        "test_real_images, test_real_labels = load_images('/kaggle/input/1000-videos-split/1000_videos/test/real', 0)  #change the path\n",
        "test_fake_images, test_fake_labels = load_images('/kaggle/input/1000-videos-split/1000_videos/test/fake', 1) #change the path\n",
        "\n",
        "# Combine real and fake datasets\n",
        "x_train = np.concatenate((train_real_images, train_fake_images), axis=0)\n",
        "y_train = np.concatenate((train_real_labels, train_fake_labels), axis=0)\n",
        "\n",
        "x_validation = np.concatenate((validation_real_images, validation_fake_images), axis=0)\n",
        "y_validation = np.concatenate((validation_real_labels, validation_fake_labels), axis=0)\n",
        "\n",
        "x_test = np.concatenate((test_real_images, test_fake_images), axis=0)\n",
        "y_test = np.concatenate((test_real_labels, test_fake_labels), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "C2g8YGl_MGRY"
      },
      "outputs": [],
      "source": [
        "# Normalize the images\n",
        "x_train = x_train / 255.0\n",
        "x_validation = x_validation / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "print(f'Training data shape: {x_train.shape}, Labels shape: {y_train.shape}')\n",
        "print(f'Validation data shape: {x_validation.shape}, Labels shape: {y_validation.shape}')\n",
        "print(f'Test data shape: {x_test.shape}, Labels shape: {y_test.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "rD5iisU_MGRY"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(Image_Size, Image_Size, Channels)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "C6hadjQUMGRZ"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=Epochs, validation_data=(x_validation, y_validation))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "afmrvb8JMGRZ"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "2yxhJGiQMGRZ"
      },
      "outputs": [],
      "source": [
        "# Define the prediction function for a video\n",
        "def pred_video(model, video_path):\n",
        "    frames = []\n",
        "    img_data = cv2.imread(video_path)\n",
        "    img_data = cv2.resize(img_data, (Image_Size, Image_Size))\n",
        "    img_data = img_data / 255.0\n",
        "    frames.append(img_data)\n",
        "    frames = np.array(frames)\n",
        "    predictions = model.predict(frames)\n",
        "    final_prediction = np.mean(predictions)\n",
        "    return final_prediction, predictions\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "7QFVRKZnMGRZ"
      },
      "outputs": [],
      "source": [
        "# Define the prediction function for individual images\n",
        "def pred(model, image):\n",
        "    image = tf.image.resize(image, [Image_Size, Image_Size])  # Resize to match the input shape\n",
        "    image = np.expand_dims(image, axis=0)  # Expand dims to match input shape\n",
        "    prediction = model.predict(image)\n",
        "    predicted_class = 'fake' if prediction[0] > 0.5 else 'real'\n",
        "    confidence = prediction[0][0] * 100 if predicted_class == 'fake' else (1 - prediction[0][0]) * 100\n",
        "    return predicted_class, confidence\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "x59slsQGMGRZ"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "import pickle\n",
        "with open('model_f_real_pickle_final', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "em9jz_SEMGRZ"
      },
      "outputs": [],
      "source": [
        "# Load the saved model using pickle\n",
        "with open('model_f_real_pickle_final', 'rb') as f:  # Update this path\n",
        "    model_saved = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "UKw_MRByMGRZ"
      },
      "outputs": [],
      "source": [
        "# Assuming you have class names\n",
        "class_names = ['fake', 'real']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "TvXdSDhPMGRZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Function to load and preprocess an image from a given path\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_image(image, channels=3)\n",
        "    image = tf.image.resize(image, [Image_Size, Image_Size])\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "VBsFVk8QMGRZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict using the saved model\n",
        "plt.figure(figsize=(15, 15))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)  # Create a dataset for testing\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        img = images[i].numpy().astype(\"uint8\")\n",
        "        img_display = img * 255.0  # Scale back to [0, 255] range\n",
        "        plt.imshow(img_display.astype(\"uint8\"))\n",
        "        predicted_class, confidence = pred(model_saved, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]]\n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence:.2f}%\")\n",
        "        plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "iWpi9JmeMGRZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Add an image of your choice\n",
        "image_path = '/kaggle/input/1000-videos-split/1000_videos/test/real/069_20.png' # Update this path\n",
        "image = load_and_preprocess_image(image_path)\n",
        "# Plot and predict for the new image\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(image.numpy())\n",
        "predicted_class, confidence = pred(model_saved, image.numpy())\n",
        "plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}%\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU5iklKNMGRZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzHvKH-AMGRZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 3989137,
          "sourceId": 6945939,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30747,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}